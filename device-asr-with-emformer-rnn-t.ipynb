{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:54:48.834181Z","iopub.execute_input":"2024-10-18T22:54:48.834644Z","iopub.status.idle":"2024-10-18T22:54:50.169004Z","shell.execute_reply.started":"2024-10-18T22:54:48.834596Z","shell.execute_reply":"2024-10-18T22:54:50.167798Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Data acquisition","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\n\n\n# The data acquisition process will stop after this number of steps.\n# This eliminates the need of process synchronization and makes this\n# tutorial simple.\nNUM_ITER = 100\n\n\ndef stream(q, format, src, segment_length, sample_rate):\n    from torchaudio.io import StreamReader\n\n    print(\"Building StreamReader...\")\n    streamer = StreamReader(src, format=format)\n    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate)\n\n    print(streamer.get_src_stream_info(0))\n    print(streamer.get_out_stream_info(0))\n\n    print(\"Streaming...\")\n    print()\n    stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\n    for _ in range(NUM_ITER):\n        (chunk,) = next(stream_iterator)\n        q.put(chunk)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:55:42.735190Z","iopub.execute_input":"2024-10-18T22:55:42.735778Z","iopub.status.idle":"2024-10-18T22:55:47.121513Z","shell.execute_reply.started":"2024-10-18T22:55:42.735731Z","shell.execute_reply":"2024-10-18T22:55:47.120099Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Building inference pipeline","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n    \"\"\"Build inference pipeline from RNNTBundle.\n\n    Args:\n        bundle (torchaudio.pipelines.RNNTBundle): Bundle object\n        beam_width (int): Beam size of beam search decoder.\n    \"\"\"\n\n    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):\n        self.bundle = bundle\n        self.feature_extractor = bundle.get_streaming_feature_extractor()\n        self.decoder = bundle.get_decoder()\n        self.token_processor = bundle.get_token_processor()\n\n        self.beam_width = beam_width\n\n        self.state = None\n        self.hypotheses = None\n\n    def infer(self, segment: torch.Tensor) -> str:\n        \"\"\"Perform streaming inference\"\"\"\n        features, length = self.feature_extractor(segment)\n        self.hypotheses, self.state = self.decoder.infer(\n            features, length, self.beam_width, state=self.state, hypothesis=self.hypotheses\n        )\n        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)\n        return transcript","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:56:55.738016Z","iopub.execute_input":"2024-10-18T22:56:55.738667Z","iopub.status.idle":"2024-10-18T22:56:55.750184Z","shell.execute_reply.started":"2024-10-18T22:56:55.738623Z","shell.execute_reply":"2024-10-18T22:56:55.748548Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ContextCacher:\n    \"\"\"Cache the end of input data and prepend the next input data with it.\n\n    Args:\n        segment_length (int): The size of main segment.\n            If the incoming segment is shorter, then the segment is padded.\n        context_length (int): The size of the context, cached and appended.\n    \"\"\"\n\n    def __init__(self, segment_length: int, context_length: int):\n        self.segment_length = segment_length\n        self.context_length = context_length\n        self.context = torch.zeros([context_length])\n\n    def __call__(self, chunk: torch.Tensor):\n        if chunk.size(0) < self.segment_length:\n            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n        chunk_with_context = torch.cat((self.context, chunk))\n        self.context = chunk[-self.context_length :]\n        return chunk_with_context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:57:14.507659Z","iopub.execute_input":"2024-10-18T22:57:14.508115Z","iopub.status.idle":"2024-10-18T22:57:14.518583Z","shell.execute_reply.started":"2024-10-18T22:57:14.508073Z","shell.execute_reply":"2024-10-18T22:57:14.517013Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## The main process","metadata":{}},{"cell_type":"markdown","source":"1. Initialize the inference pipeline.\r\n2. \r\nLaunch data acquisition subproces\n3. Run inference.\n4. Clean upean up","metadata":{}},{"cell_type":"code","source":"!pip install pathos joblib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:05:29.771851Z","iopub.execute_input":"2024-10-18T23:05:29.772308Z","iopub.status.idle":"2024-10-18T23:05:43.960657Z","shell.execute_reply.started":"2024-10-18T23:05:29.772256Z","shell.execute_reply":"2024-10-18T23:05:43.958941Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (1.4.2)\nRequirement already satisfied: ppft>=1.7.6.9 in /opt/conda/lib/python3.10/site-packages (from pathos) (1.7.6.9)\nRequirement already satisfied: dill>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from pathos) (0.3.9)\nRequirement already satisfied: pox>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from pathos) (0.3.5)\nRequirement already satisfied: multiprocess>=0.70.17 in /opt/conda/lib/python3.10/site-packages (from pathos) (0.70.17)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def main(device, src, bundle):\n    print(torch.__version__)\n    print(torchaudio.__version__)\n\n    print(\"Building pipeline...\")\n    pipeline = Pipeline(bundle)\n\n    sample_rate = bundle.sample_rate\n    segment_length = bundle.segment_length * bundle.hop_length\n    context_length = bundle.right_context_length * bundle.hop_length\n\n    print(f\"Sample rate: {sample_rate}\")\n    print(f\"Main segment: {segment_length} frames ({segment_length / sample_rate} seconds)\")\n    print(f\"Right context: {context_length} frames ({context_length / sample_rate} seconds)\")\n\n    cacher = ContextCacher(segment_length, context_length)\n\n    @torch.inference_mode()\n    def infer():\n        for _ in range(NUM_ITER):\n            chunk = q.get()\n            segment = cacher(chunk[:, 0])\n            transcript = pipeline.infer(segment)\n            print(transcript, end=\"\\r\", flush=True)\n\n    import torch.multiprocessing as mp\n\n    ctx = mp.get_context(\"spawn\")\n    q = ctx.Queue()\n    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n    p.start()\n    infer()\n    p.join()\n\n\nif __name__ == \"__main__\":\n    main(\n        device=\"avfoundation\",\n        src=\":1\",\n        bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:05:43.963613Z","iopub.execute_input":"2024-10-18T23:05:43.964213Z"}},"outputs":[{"name":"stdout","text":"2.4.0+cpu\n2.4.0+cpu\nBuilding pipeline...\nSample rate: 16000\nMain segment: 2560 frames (0.16 seconds)\nRight context: 640 frames (0.04 seconds)\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/opt/conda/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/opt/conda/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n    self = reduction.pickle.load(from_parent)\nAttributeError: Can't get attribute 'stream' on <module '__main__' (built-in)>\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}